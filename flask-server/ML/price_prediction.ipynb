{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  condo_df = pd.read_csv('../dataset/cleaned/bangkok-condo-dataset.csv')\n",
    "  return condo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_df = load_data()\n",
    "condo_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_df['Condo_Area_corr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unsed columns\n",
    "\n",
    "cols = ['ID', 'Condo_NAME_EN', 'Condo_NAME_TH', 'Condo_link', 'All_Data', 'Condo_Area_corr']\n",
    "condo_df = condo_df.drop(cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing value\n",
    "(condo_df == '').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_df = condo_df.applymap(lambda x: np.nan if x == '' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The missing data for floor feature can be filled by hand\n",
    "\n",
    "condo_df.loc[condo_df['#_Floor'].isna(), :'#_Floor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_df.loc[1427, '#_Floor'] = 27\n",
    "condo_df.loc[1428, '#_Floor'] = 5\n",
    "condo_df.loc[[1427, 1428], :'#_Floor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot Area_m2 (condo price per sqm)\n",
    "\n",
    "condo_df['Area_m2'] = condo_df['Area_m2'].str.replace(',', '')\n",
    "condo_df['Area_m2'] = condo_df['Area_m2'].fillna(-1).astype('int64').replace(-1, np.nan)\n",
    "\n",
    "plt.figure(figsize=(5,10))\n",
    "_, bp = condo_df.boxplot('Area_m2', return_type='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 = condo_df['Area_m2'].quantile(0.25); # Q3 = condo_df['Area_m2'].quantile(0.75)\n",
    "# IQR = Q3-Q1\n",
    "# Outlier > Q3+1.5*IQR, Outlier < Q1-1.5*IQR\n",
    "\n",
    "outliers = [flier.get_ydata() for flier in bp[\"fliers\"]][0]\n",
    "boxes = [box.get_ydata() for box in bp[\"boxes\"]][0]\n",
    "medians = [median.get_ydata() for median in bp[\"medians\"]][0]\n",
    "whiskers = [whiskers.get_ydata() for whiskers in bp[\"whiskers\"]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In which year that the outlier-condo were built?\n",
    "\n",
    "outlier_condo = condo_df[condo_df['Area_m2'] > min(outliers)]['Year_built'].value_counts().sort_index()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Built by year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('frequency')\n",
    "ax = outlier_condo.plot.bar()\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.0025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Area_m2\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Histogram of Area_m2')\n",
    "plt.xlabel('Area_m2')\n",
    "ax = condo_df['Area_m2'].plot.hist(bins=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Sale_Price_Inc[Year]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Histogram of Sale_Price_Inc[Year]')\n",
    "plt.xlabel('Sale_Price_Inc[Year]')\n",
    "ax = condo_df['Sale_Price_Inc[Year]'].plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace area_m2 and Sale_Price_Inc[Year] with median as the data are very skewed\n",
    "\n",
    "condo_df['Area_m2'] = condo_df['Area_m2'].fillna(condo_df['Area_m2'].median())\n",
    "condo_df['Sale_Price_Inc[Year]'] = condo_df['Sale_Price_Inc[Year]'].fillna(condo_df['Sale_Price_Inc[Year]'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Rental_Yield_Inc[Year] as it has many missing value\n",
    "\n",
    "condo_df = condo_df.drop('Rental_Yield_Inc[Year]', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add kind of condo\n",
    "\n",
    "condo_df['Kind'] = condo_df['#_Floor'].apply(lambda x: 'high rise' if x > 9 else 'low rise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_df['Address_TH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = condo_df['Address_TH'].apply(lambda x: re.findall(r'(ถนน\\s?[ก-๙]+\\s?\\d?)', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add road\n",
    "\n",
    "import re\n",
    "\n",
    "condo_df['Road'] = condo_df['Address_TH'].apply(lambda x: re.findall(r'(ถนน\\s?[ก-๙]+\\s?\\d?)', x))\n",
    "condo_df['Road'] = condo_df['Road'].apply(lambda x: x[0] if len(x)> 0 else np.nan)\n",
    "condo_df['Road'] = condo_df['Road'].str.replace(\" \", \"\").str.strip()\n",
    "condo_df['Road'] = condo_df['Road'].fillna(np.nan)\n",
    "condo_df = condo_df.drop('Address_TH', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_df['Road'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation between features\n",
    "\n",
    "correlation = round(condo_df.corr(numeric_only=True), 2)\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "plt.subplots(figsize=(11, 9))\n",
    "sns.heatmap(correlation, cmap=cmap, linewidths=.5, annot=True, vmin=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_df = condo_df.drop('Sale_Price_Inc[Year]', axis=1)\n",
    "condo_df = condo_df.drop('Sale_Price_Increment[Quarter]', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(condo_df['Year_built'], condo_df['Sale_Price_Sqm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_df['Road']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering using one hot encoding\n",
    "\n",
    "new_area = pd.get_dummies(condo_df['Condo_area'], dummy_na=True, prefix='Area_')\n",
    "new_kind = pd.get_dummies(condo_df['Kind'], dummy_na=True, prefix='Kind_')\n",
    "new_road = pd.get_dummies(condo_df['Road'], dummy_na=True, prefix='Road_')\n",
    "condo_df = pd.concat([condo_df, new_area, new_kind, new_road], axis = 1)\n",
    "\n",
    "condo_df = condo_df.drop('Condo_area', axis=1)\n",
    "condo_df = condo_df.drop('Kind', axis=1)\n",
    "condo_df = condo_df.drop('Road', axis=1)\n",
    "condo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function clean_data(df) to do all cleaning\n",
    "def clean_data(df):\n",
    "  \n",
    "  \n",
    "\n",
    "  #drop unused columns\n",
    "  cols = ['ID', 'Condo_NAME_EN', 'Condo_NAME_TH', 'Condo_link', 'All_Data', 'Condo_Area_corr']\n",
    "  clean_df = df.drop(cols, axis=1)\n",
    "\n",
    "  #replace '' with np.nan\n",
    "  clean_df = clean_df.applymap(lambda x: np.nan if x == '' else x)\n",
    "\n",
    "  #fill no.of Floor by hand\n",
    "  clean_df.loc[1427, '#_Floor'] = 27\n",
    "  clean_df.loc[1428, '#_Floor'] = 5\n",
    "\n",
    "  #clean Area_m2 columns\n",
    "  clean_df['Area_m2'] = clean_df['Area_m2'].str.replace(',', '')\n",
    "  clean_df['Area_m2'] = clean_df['Area_m2'].fillna(-1).astype('int64').replace(-1, np.nan)\n",
    "\n",
    "  #replace missing data with median as the data are very skewed\n",
    "  clean_df['Area_m2'] = clean_df['Area_m2'].fillna(clean_df['Area_m2'].median())\n",
    "  clean_df['Sale_Price_Inc[Year]'] = clean_df['Sale_Price_Inc[Year]'].fillna(clean_df['Sale_Price_Inc[Year]'].median())\n",
    "\n",
    "  #drop Rental_Yield_Inc[Year] as it has many missing value\n",
    "  clean_df = clean_df.drop('Rental_Yield_Inc[Year]', axis=1)\n",
    "\n",
    "  #add kind of condo [>9 floor: high rise, else: low rise]\n",
    "  clean_df['Kind'] = clean_df['#_Floor'].apply(lambda x: 'high rise' if x > 9 else 'low rise')\n",
    "\n",
    "  #add road feature\n",
    "  clean_df['Road'] = clean_df['Address_TH'].apply(lambda x: re.findall(r'(ถนน\\s?[\\u0E00-\\u0E7F]+\\s?\\d?)', x))\n",
    "  clean_df['Road'] = clean_df['Road'].apply(lambda x: x[0] if len(x)> 0 else np.nan)\n",
    "  clean_df['Road'] = clean_df['Road'].str.replace(\" \", \"\").str.strip()\n",
    "  clean_df['Road'] = clean_df['Road'].fillna(np.nan)\n",
    "  clean_df = clean_df.drop('Address_TH', axis=1)\n",
    "\n",
    "  #drop Sale_Price_Inc[Year], Sale_Price_Increment[Quarter]\n",
    "  clean_df = clean_df.drop('Sale_Price_Inc[Year]', axis=1)\n",
    "  clean_df = clean_df.drop('Sale_Price_Increment[Quarter]', axis=1)\n",
    "\n",
    "  #get_dummies\n",
    "  new_area = pd.get_dummies(clean_df['Condo_area'], dummy_na=True, prefix='Area_')\n",
    "  new_kind = pd.get_dummies(clean_df['Kind'], dummy_na=True, prefix='Kind_')\n",
    "  new_road = pd.get_dummies(clean_df['Road'], dummy_na=True, prefix='Road_')\n",
    "  clean_df = pd.concat([clean_df, new_area, new_kind, new_road], axis = 1)\n",
    "\n",
    "  clean_df = clean_df.drop('Condo_area', axis=1)\n",
    "  clean_df = clean_df.drop('Kind', axis=1)\n",
    "  clean_df = clean_df.drop('Road', axis=1)\n",
    "\n",
    "  return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run clean_data function\n",
    "condo_df = load_data()\n",
    "clean_df = clean_data(condo_df)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = condo_df['Sale_Price_Sqm'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to split, extract feature and scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def create_dataset(df, random_state=42):\n",
    "  y = df['Sale_Price_Sqm'].values\n",
    "  X = df.drop('Sale_Price_Sqm', axis=1).copy()\n",
    "  \n",
    "  # y = MinMaxScaler().fit_transform(y.values.reshape(-1,1)).ravel()\n",
    "  # y = MinMaxScaler().inverse_transform(y) to convert value back\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "  X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=random_state)\n",
    "\n",
    "  # Minmax Scaler\n",
    "  scaler = MinMaxScaler()\n",
    "\n",
    "  # Fit scaler on training data and transform on other dataset to prevent information leakage\n",
    "  X_train = scaler.fit_transform(X_train)\n",
    "  X_val = scaler.transform(X_val)\n",
    "  X_test = scaler.transform(X_test)\n",
    "\n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, scaler = create_dataset(clean_df)\n",
    "print(f\"Train size: {X_train.shape[0]}\")\n",
    "print(f\"Validation size: {X_val.shape[0]}\")\n",
    "print(f\"Test size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler ไว้สำหรับใช้ใน heroku\n",
    "joblib.dump(scaler, \"data_scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to split, extract feature and scale \n",
    "# using StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def create_dataset_standard(df, random_state=42):\n",
    "  y = df['Sale_Price_Sqm'].values\n",
    "  X = df.drop('Sale_Price_Sqm', axis=1).copy()\n",
    "  \n",
    "  # y = MinMaxScaler().fit_transform(y.values.reshape(-1,1)).ravel()\n",
    "  # y = MinMaxScaler().inverse_transform(y) to convert value back\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "  X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=random_state)\n",
    "\n",
    "  # Minmax Scaler\n",
    "  scaler = StandardScaler()\n",
    "\n",
    "  # Fit scaler on training data and transform on other dataset to prevent information leakage\n",
    "  # Scale only numerical data \n",
    "  num_cols = ['Year_built',\t'Area_m2',\t'#_Tower',\t'#_Floor',\t'Rental_Yield',\t'Latitude',\t'Longtitude',\t'MinDist_Station']\n",
    "  X_train = scaler.fit_transform(X_train[num_cols])\n",
    "  X_val = scaler.transform(X_val[num_cols])\n",
    "  X_test = scaler.transform(X_test[num_cols])\n",
    "\n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout\n",
    "# from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.losses import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "import kerastuner as kt\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function for RMSE loss evaluation\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred): # for keras\n",
    "  result = K.sqrt(mean_squared_error(y_true, y_pred))\n",
    "  return result\n",
    "\n",
    "def rmse(y_true, y_pred): # for sklearn\n",
    "  result = mse(y_true, y_pred, squared=False)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add NN model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(60, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "#model.add(Dropout(0.25)) keep this for later optimization\n",
    "model.add(Dense(30, activation='relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.build(X_train.shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complile model\n",
    "\n",
    "model.compile(loss=root_mean_squared_error, optimizer='adam', metrics='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=16,  verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE Loss\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Model\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    for i in range(hp.Int('num_layers', 2, 5)):\n",
    "        model.add(Dense(units=hp.Int('units_' + str(i),\n",
    "                                     min_value=16,\n",
    "                                     max_value=64,\n",
    "                                     step=16),\n",
    "                        activation='relu'))\n",
    "       \n",
    "        # drop_rate = hp.Choice('drop_rate_' + str(i), [0.0, 0.1, 0.2,0.3])\n",
    "        # model.add(Dropout(rate=drop_rate))\n",
    "    #model.add(Dense(1, activation='linear'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss=root_mean_squared_error,\n",
    "        metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune NN\n",
    "\n",
    "def nn_tunner(build_model):\n",
    "  # Instantiate model tuner\n",
    "  tuner = kt.RandomSearch(\n",
    "      build_model,\n",
    "      objective='mse',\n",
    "      max_trials=10,\n",
    "      seed = 42,\n",
    "      directory = 'tuner')\n",
    "\n",
    "  # Search summary\n",
    "  print('\\n############')\n",
    "  print('\\nTuning Summary')\n",
    "  print(tuner.search_space_summary())\n",
    "\n",
    "  # Initialize search space\n",
    "  print('\\n############')\n",
    "  print('\\nTuning Model')\n",
    "  tuner.search(X_train, y_train,\n",
    "              epochs=100,\n",
    "              validation_data=(X_val, y_val))\n",
    "  \n",
    "  return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = nn_tunner(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned hyperparameters\n",
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.build(X_train.shape)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE of tuned model\n",
    "y_hat = best_model.predict(X_val)\n",
    "rmse(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state=2020)\n",
    "\n",
    "# Generate tuning parameters\n",
    "n_estimators = np.arange(200,1000,200)\n",
    "max_depth = np.arange(2,int(X_train.shape[1]/10) ,3)\n",
    "min_samples_split = np.arange(2,5,1)\n",
    "min_samples_leaf = np.arange(2,5,1)\n",
    "\n",
    "rf_params = dict(n_estimators = n_estimators,  \n",
    "               min_samples_split = min_samples_split, \n",
    "               max_depth = max_depth,\n",
    "               min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "# Instantiate grid search with RMSE as scoring\n",
    "rf_grid = GridSearchCV(rf, param_grid=rf_params, cv = 3, verbose = 1, n_jobs = -1, scoring='neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned = rf_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average RMSE in cross validation\n",
    "-np.nanmean(rf_grid.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE from validation set\n",
    "yhat_val = rf_tuned.best_estimator_.predict(X_val)\n",
    "rmse(y_val, yhat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model\n",
    "cls = rf_tuned.best_estimator_\n",
    "joblib.dump(cls, 'rf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_best_model = joblib.load('rf.joblib')\n",
    "predictions = nn_best_model.predict(X_val)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads, dumps\n",
    "\n",
    "# create function clean_data(df) to do all cleaning\n",
    "def clean_data(df):\n",
    "\n",
    "  #drop unused columns\n",
    "  cols = ['ID', 'Condo_NAME_EN', 'Condo_NAME_TH', 'Condo_link', 'All_Data', 'Condo_Area_corr']\n",
    "  clean_df = df.drop(cols, axis=1)\n",
    "\n",
    "  #replace '' with np.nan\n",
    "  clean_df = clean_df.applymap(lambda x: np.nan if x == '' else x)\n",
    "\n",
    "  #fill no.of Floor by hand\n",
    "  clean_df.loc[1427, '#_Floor'] = 27\n",
    "  clean_df.loc[1428, '#_Floor'] = 5\n",
    "\n",
    "  #clean Area_m2 columns\n",
    "  clean_df['Area_m2'] = clean_df['Area_m2'].str.replace(',', '')\n",
    "  clean_df['Area_m2'] = clean_df['Area_m2'].fillna(-1).astype('int64').replace(-1, np.nan)\n",
    "\n",
    "  #replace missing data with median as the data are very skewed\n",
    "  clean_df['Area_m2'] = clean_df['Area_m2'].fillna(clean_df['Area_m2'].median())\n",
    "  clean_df['Sale_Price_Inc[Year]'] = clean_df['Sale_Price_Inc[Year]'].fillna(clean_df['Sale_Price_Inc[Year]'].median())\n",
    "\n",
    "  #drop Rental_Yield_Inc[Year] as it has many missing value\n",
    "  clean_df = clean_df.drop('Rental_Yield_Inc[Year]', axis=1)\n",
    "\n",
    "  #add kind of condo [>9 floor: high rise, else: low rise]\n",
    "  clean_df['Kind'] = clean_df['#_Floor'].apply(lambda x: 'high rise' if x > 9 else 'low rise')\n",
    "\n",
    "  #add road feature\n",
    "  clean_df['Road'] = clean_df['Address_TH'].apply(lambda x: re.findall(r'(ถนน\\s?[\\u0E00-\\u0E7F]+\\s?\\d?)', x))\n",
    "  clean_df['Road'] = clean_df['Road'].apply(lambda x: x[0] if len(x)> 0 else np.nan)\n",
    "  clean_df['Road'] = clean_df['Road'].str.replace(\" \", \"\").str.strip()\n",
    "  clean_df['Road'] = clean_df['Road'].fillna(np.nan)\n",
    "  clean_df = clean_df.drop('Address_TH', axis=1)\n",
    "\n",
    "  #drop Sale_Price_Inc[Year], Sale_Price_Increment[Quarter]\n",
    "  clean_df = clean_df.drop('Sale_Price_Inc[Year]', axis=1)\n",
    "  clean_df = clean_df.drop('Sale_Price_Increment[Quarter]', axis=1)\n",
    "\n",
    "  #get_dummies\n",
    "  new_area = pd.get_dummies(clean_df['Condo_area'], dummy_na=True, prefix='Area_')\n",
    "  new_kind = pd.get_dummies(clean_df['Kind'], dummy_na=True, prefix='Kind_')\n",
    "  new_road = pd.get_dummies(clean_df['Road'], dummy_na=True, prefix='Road_')\n",
    "  clean_df = pd.concat([clean_df, new_area, new_kind, new_road], axis = 1)\n",
    "\n",
    "  clean_df = clean_df.drop('Condo_area', axis=1)\n",
    "  clean_df = clean_df.drop('Kind', axis=1)\n",
    "  clean_df = clean_df.drop('Road', axis=1)\n",
    "\n",
    "  return clean_df\n",
    "\n",
    "# create function to split, extract feature and scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def create_dataset(df, random_state=42):\n",
    "  y = df['Sale_Price_Sqm'].values\n",
    "  X = df.drop('Sale_Price_Sqm', axis=1).copy()\n",
    "  \n",
    "  # y = MinMaxScaler().fit_transform(y.values.reshape(-1,1)).ravel()\n",
    "  # y = MinMaxScaler().inverse_transform(y) to convert value back\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "  X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=random_state)\n",
    "\n",
    "  # Minmax Scaler\n",
    "  scaler = MinMaxScaler()\n",
    "\n",
    "  # Fit scaler on training data and transform on other dataset to prevent information leakage\n",
    "  X_train = scaler.fit_transform(X_train)\n",
    "  # print(X_val)\n",
    "  X_val = scaler.transform(X_val)\n",
    "  X_test = scaler.transform(X_test)\n",
    "\n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test, scaler\n",
    "\n",
    "#run clean_data function\n",
    "condo_df = load_data()\n",
    "clean_df = clean_data(condo_df)\n",
    "clean_df.head()\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, scaler = create_dataset(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json = [{\n",
    "        \"Condo_area\": \"Bang Kapi\",\n",
    "        \"Year_built\": 2011,\n",
    "        \"Area_m2\": 6476.0,\n",
    "        \"#_Tower\": 2,\n",
    "        \"#_Floor\": 8.0,\n",
    "        \"Sale_Price_Sqm\": 52065,\n",
    "        \"Rental_Yield\": 4.86,\n",
    "        \"Latitude\": 13.766348,\n",
    "        \"Longtitude\": 100.649395,\n",
    "        \"MinDist_Station\": 8256,\n",
    "        \"Kind\": \"low rise\",\n",
    "        \"Road\": \"\\u0e16\\u0e19\\u0e19\\u0e40\\u0e2a\\u0e23\\u0e35\\u0e44\\u0e17\\u0e22\"\n",
    "    }]\n",
    "\n",
    "input_df = pd.DataFrame.from_dict(input_json)\n",
    "\n",
    "#get_dummies\n",
    "new_area = pd.get_dummies(input_df['Condo_area'], dummy_na=True, prefix='Area_')\n",
    "new_kind = pd.get_dummies(input_df['Kind'], dummy_na=True, prefix='Kind_')\n",
    "new_road = pd.get_dummies(input_df['Road'], dummy_na=True, prefix='Road_')\n",
    "input_df = pd.concat([input_df, new_area, new_kind, new_road], axis = 1)\n",
    "\n",
    "input_df = input_df.drop('Condo_area', axis=1)\n",
    "input_df = input_df.drop('Kind', axis=1)\n",
    "input_df = input_df.drop('Road', axis=1)\n",
    "\n",
    "scaler_model = joblib.load('data_scaler.joblib')\n",
    "# print(scaler_model.feature_names_in_)\n",
    "print(input_df)\n",
    "full_df = pd.DataFrame(columns=scaler_model.feature_names_in_)\n",
    "print(input_df.iloc[0,:])\n",
    "full_df.loc[0] = input_df.iloc[0,:]\n",
    "full_df = full_df.fillna(False)\n",
    "print(full_df)\n",
    "\n",
    "X = scaler_model.transform(full_df)\n",
    "print(X)\n",
    "\n",
    "rf_best_model = joblib.load('rf.joblib')\n",
    "predictions = rf_best_model.predict(X)\n",
    "print(predictions[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
